{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n16092022 - 18092022\\nБруцкий-Стемпковский\\n\\nv.01 Частотный анализ текста.\\n\\nПрограмма реализует подсчёт предложений избегая сокращения и количество слов в них. Очистка текста от стоп-слов. \\nОбъединение слов на основе схожести. Схожесть опрееляется на основании коэффициента Танимото и расстояния Левенштейна.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "16092022 - 18092022\n",
    "Бруцкий-Стемпковский\n",
    "\n",
    "v.01 Частотный анализ текста.\n",
    "\n",
    "Программа реализует подсчёт предложений избегая сокращения и количество слов в них. Очистка текста от стоп-слов. \n",
    "Объединение слов на основе схожести. Схожесть опрееляется на основании коэффициента Танимото и расстояния Левенштейна.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве текста выбрано введение к первому тому \"Капитала\" К. Маркса. Размер исходного текста превышает 60 000 символов. Написан на русском языке, встречаются слова на латинице. Содержит различные знаки препинания, ссылки в различных скобках, библиотечные шифры, и прочее.\n",
    "\n",
    "В качестве вспомогательного материала использовал материал из интернета:\n",
    "1. https://habr.com/ru/post/517410/\n",
    "2. https://grishaev.me/2012/10/05/1/\n",
    "3. https://tirinox.ru/levenstein-python/\n",
    "4. https://pymorphy2.readthedocs.io/en/stable/\n",
    "\n",
    "Импортируем необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import time\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем файл, прочтём его содержимое в переменную text, которая будет содержать текст, закроем файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_obj = open(\"text.txt\")\n",
    "text = \"\"\n",
    "text = file_obj.read()\n",
    "file_obj.close()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сперва пройдём весь текст и избавимся от большинства знаков препинания, цифр, скобок, знаков табуляции и новой строки. Но так как нам требуется разделить текст на предложения, не будем убирать знаки окончания предложения (точки, знак вопроса, восклицательный знак). Также не будем пока переводить все буквы в нижний регистр. Планирую использовать тот факт, что при использовании точки для сокращения слова (т. д., проч. и другое) следующее за ним слово пишется с маленькой буквы. Такая точка не считается принаком завершения предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~0123456789\\n\\t\\'\"”“–'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_symbol = string.punctuation\n",
    "spec_symbol = spec_symbol.replace(\".\", \"\")\n",
    "spec_symbol = spec_symbol.replace(\"!\", \"\")\n",
    "spec_symbol = spec_symbol.replace(\"?\", \"\")\n",
    "spec_symbol += string.digits\n",
    "spec_symbol += \"\\n\\t\\'\\\"”“–\"\n",
    "symbols_of_end_sentence = \".!?\"\n",
    "\n",
    "spec_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем замену всех спецсимволов на знак пробела (обозначается символом \\x20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64332"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for symbol in spec_symbol:\n",
    "    text = text.replace(symbol, \"\\x20\")\n",
    "text = text.replace(\"...\", \"\\x20\")\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдем весь текст от начала и до конца в цикле. Если встретим символ конца строки, проверим регистр ближайшего следующего за ним символа. Введем логическую переменную meeting_symbol, которая принимает значение True если мы недавно встретили знак конца предложения, но еще не \"решили его судьбу\". Будем заменять \"правильные\" символы конца строки на символ \"*\" (их там уже нет, хотя можно бы и более специфический символ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_symbol = False\n",
    "symbol_of_end_sentence = None\n",
    "\n",
    "for i in range(len(text)):\n",
    "\n",
    "    if text[i] in symbols_of_end_sentence:\n",
    "        meeting_symbol = True\n",
    "        symbol_of_end_sentence = text[i]\n",
    "        continue\n",
    "\n",
    "    if meeting_symbol == True and text[i] != \"\\x20\":\n",
    "        if text[i] == text[i].upper():\n",
    "            text = text.replace(symbol_of_end_sentence, \"\\x20\" + \"*\", 1)\n",
    "            meeting_symbol = False\n",
    "            symbol_of_end_sentence = None\n",
    "        else:\n",
    "            text = text.replace(symbol_of_end_sentence, \"\\x20\", 1)\n",
    "            meeting_symbol = False\n",
    "            symbol_of_end_sentence = None\n",
    "\n",
    "#выловим последний знак препинания, т.к. за ним ничего нет\n",
    "for _ in symbols_of_end_sentence:\n",
    "    text = text.replace(_, \"\\x20\" + \"*\", 1)\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список из строки для подсчёта количество предложений и слов в них. Список бует содержать слова в виде строк и символ \"*\". Последний будет использован как разделитель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8813"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = text.split()\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим количество предложений и слов в них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_sentence = text_list.count(\"*\")\n",
    "word_counter = 0\n",
    "list_of_words_in_sentence = []\n",
    "\n",
    "for word in text_list:\n",
    "    if word == \"*\":\n",
    "        list_of_words_in_sentence.append(word_counter)\n",
    "        word_counter = 0\n",
    "    else:\n",
    "        word_counter += 1\n",
    "\n",
    "number_of_sentence, len(list_of_words_in_sentence)\n",
    "#list_of_words_in_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся обрано к строковой переменной. Удалим символы конца предложения, выровняем регистр. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace(\"*\", \"\")\n",
    "text = text.lower()\n",
    "\n",
    "len(text)\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь создадим список из слов текста разделяя их по пробелам. В этот раз в списке будут лишь слова без каких-либо знаков препинания. Но у этого списка две пробемы:\n",
    "1. Содержатся стоп-слова (предлоги, союзы, междометия и проч., не несут смысловой нагрузки.)\n",
    "2. \"Нормальные\" слова, несущие смысловую нагрузку, употреблены в различных формах. (Производства, производстве, производящие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8413"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = text.split()\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим все слова, состоящие из 1 - 3 символов. Так мы отсеем большинство предлогов и союзов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [_ for _ in text_list if len(_) > 3]\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем список стоп-слов из библиотеки NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "russian_stopwords = nltk.corpus.stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "\n",
    "stop_words = open(\"stop_words.txt\", \"w\")\n",
    "for _ in russian_stopwords:\n",
    "    stop_words.write(_ + \"\\n\")\n",
    "stop_words.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим стоп-слова из нашего списка слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5455"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [_ for _ in text_list if not _ in russian_stopwords]\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем две функции, дающие метрику схожести двух слов. @lru_cache - кэш (память) вызовов функции. Из-за рекурсивного вызова функции самой себя она многократно выполняет свою работу при абсолютно одинаковых входных данных. Преставленный здесь вариант является \"средним\" по сложности вариантом из [3]. У нас нет необходимости кэшировать только последние вызова функции, т.к. входные данные - уже подготовленные слова, а не тексты 10^6 - 10^7 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanimoto(word_a, word_b):\n",
    "    a, b, c = len(word_a), len(word_a), 0\n",
    "    for symbol in word_a:\n",
    "        if symbol in word_b:\n",
    "            c += 1\n",
    "    return c/(a + b - c)\n",
    "\n",
    "def levenstein(a, b):\n",
    "    @lru_cache(maxsize=len(a) * len(b))\n",
    "    def recursive(i, j):\n",
    "        if i == 0 or j == 0:\n",
    "            return max(i, j)\n",
    "        elif a[i - 1] == b[j - 1]:\n",
    "            return recursive(i - 1, j - 1)\n",
    "        else:\n",
    "            return 1 + min(\n",
    "                recursive(i, j - 1),\n",
    "                recursive(i - 1, j),\n",
    "                recursive(i - 1, j - 1)\n",
    "            )\n",
    "    return recursive(len(a), len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методом \"Подбора\" определим критерий схожести слов. При tanimoto >= 0.5 и levenstein <= 4. Слова считаются одинаковыми. Второе в таком случае заменяется на первое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_before = open(\"before.txt\", \"w\")\n",
    "for _ in text_list:\n",
    "    text_before.write(_ + \"\\n\")\n",
    "text_before.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью двух вложенных списков сравним каждое слово с каждым... (очень плохая идея =D, работало 3 минуты 20 сек). Выполним замены. Расположим функцию tanimoto первой, т.к. она быстрее рассчитывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_list)):\n",
    "    for j in range(i+1, len(text_list)):\n",
    "        if tanimoto(text_list[i], text_list[j]) >= 0.5 and levenstein(text_list[i], text_list[j]) <= 4:\n",
    "            text_list[i], text_list[j] = text_list[i], text_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_after = open(\"after.txt\", \"w\")\n",
    "for _ in text_list:\n",
    "    text_after.write(_ + \"\\n\")\n",
    "text_after.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим множество слов в тексте, подсчитаем количество вхождений каждого слова в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_set = set(text_list)\n",
    "\n",
    "len(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_words = {}\n",
    "for _ in words_set:\n",
    "    if text_list.count(_) in dict_of_words:\n",
    "        dict_of_words[text_list.count(_)].append(_)\n",
    "    else:\n",
    "        dict_of_words[text_list.count(_)] = [_]\n",
    "\n",
    "dict_of_words_file = open(\"dict_of_words_file.txt\", \"w\")\n",
    "for _ in sorted(dict_of_words.keys(), reverse = True):\n",
    "    dict_of_words_file.write(str(_) + \"\\x20\" + \"\\x20\".join(dict_of_words[_]) + \"\\n\")\n",
    "dict_of_words_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
